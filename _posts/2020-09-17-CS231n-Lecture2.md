
---
layout: post
title: CS231n 강의 요약 (2) - Image Classification
categories: ["DL"]
tags: ["CS231n"]
---

## CS231n 강의 요약 (2) - Image Classification

지난 시간에 배웠던 MIT의 The Summer Vision Project의 목표는 Image Classification이였습니다. 오늘은 Image Classification이 무엇인지, 어떤 방식으로 하는지에 대해 알아보도록 하겠습니다.

<br>

### Image Classification의 목표

![Image Classification](https://drive.google.com/uc?export=view&id=1ivSIoWOsAaHK3eUmQIkFgyQPRP8GZAaO)

우선, Image Classification은 어떤 주어진 사진이 있을 때 그 사진이 무엇을 뜻하는 사진인지 알아맞추는 것입니다.
사람들은 왼쪽에 있는 사진을 보고 [~~떼껄룩~~](https://www.youtube.com/watch?v=9yD7MB3TiA0) 고양이라는 것을 쉽게 알아차릴 수 있죠. 그러나 컴퓨터에게는 그렇지가 않습니다.

![Cat 1](https://drive.google.com/uc?export=view&id=1sUSdt5jFHfDwBcz1p8gu0PdN7pOBrCll)

컴퓨터에 저장된 고양이는 사실 엄청나게 많은 숫자들의 집합입니다. 만약 사진이 600 * 800 사이즈의 RGB 3채널의 이미지라면 600 * 800 * 3짜리 배열입니다.

![Cat 2](https://drive.google.com/uc?export=view&id=1iNrHv2YkD5czInPW_zyUBs1iJ9FK65z3)
![Cats](https://drive.google.com/uc?export=view&id=1iUpUY8zEyBN37lbBRgRyGStHkMlSuAvN)

만약에 고양이가 살짝 움직인 0.5초 뒤에 찍은 사진은 어떨까요? 숫자들의 배열은 **"고양이성"**이 하나도 없기 떄문에 아주 살짝만 움직여도 배열의 숫자들이 전부 바뀝니다.
그렇지만 사람의 눈에는 두 사진은 거의 차이가 없다고 느끼죠. 이렇기 때문에 컴퓨터는 사진을 인식하기가 굉장히 어렵습니다. 이는 같은 고양이를 조금 어두운 장소에서 찍거나,
똑같이 고양이이지만 종이 다른 고양이일 경우, 배경때문에 잘 구분이 안가는 경우, 숨어있는 경우 등등에도 마찬가지로 사진 배열에 저장된 숫자들은 천차만별입니다.

<br>

### Data Driven

![Data Driven](https://drive.google.com/uc?export=view&id=1fs3SX_rABf0wBnEd0VYa7VlRPJFcXc1E)
이러한 이유로 어떤 사진에 대해서 일반적인 알고리즘을 적용하여 그 사진이 어떤 사진인지 판별하기는 굉장히 어렵습니다. 고양이의 예시에서도 고양이가 모두 같은 포즈를 하고있는 것도 아니고, 세상에 고양이가 단 한마리 있는 것도 아니기 때문에 이러한 방식은 사실상 불가능합니다.

그렇기 때문에 등장한 방식이 바로 Data-Driven, 즉 데이터로부터 이끌어낸 방식을 사용할 것입니다. 많은 데이터를 수집하여 특정한 방식으로 데이터들을 분류하고 이들을 활용하여 머신러닝 모델을 학습시킬 것 입니다. 이렇게 학습된 모델을 새로운 이미지에 적용하여 이것이 어떤 사진인지 판별하게 하는 것입니다. 이런 방식은 Deep Learning뿐만 아니라 일반적인 머신러닝에서도 통용되는 방식입니다.

<br>

### Nearest Neighbor

![NN](https://drive.google.com/uc?export=view&id=14Osur8K5CdJRiLDYbd761r9HU_g91ggK)

사진들을 분류할 수 있는 Classifier들 중 가장 간단한 방식부터 소개를 하겠습니다. 바로 **Nearest Neighbor**, 직역하면 가장 가까운 이웃입니다. 이름에서 알 수 있듯, 최대한 거리가 가까운 사진들을 모으는 방식으로 가장 간단한 만큼 성능은 좋지 않은 방식입니다.  

사진을 예시로 보면 비스무리하게 생긴 이미지들이 한 분류로 묶여있는 것을 알 수 있습니다. 그러나 실제로 자세히 보면 전혀 다른 이미지가 구도나 배경이 비슷하기 때문에 같은 분류로 묶여있습니다. ~~개구리와 전투기가 같이 묶여있네요..~~ 이렇게 눈으로 보면 이해를 하겠지만 실제로 컴퓨터는 사진이 비슷하다는 것을 어떻게 인식할까요?

![L1](https://drive.google.com/uc?export=view&id=1xm46KwmqY6IH1CeQIKGKYqKoK425cQ5q)

바로 이미지들 사이의 L1 Distance, 혹은 [Manhatten Distance](https://ko.wikipedia.org/wiki/%EB%A7%A8%ED%95%B4%ED%8A%BC_%EA%B1%B0%EB%A6%AC)를 계산하여 이를 비교합니다. 두 이미지의 같은 위치에 있는 픽셀값의 차이의 절댓값을 전부 더한다는 의미입니다. 이 방식으로는 실제로 두 이미지간의 유사도를 나타내기에는 턱없이 부족하지만 Nearest Neighbor에서는 말 그대로 가장 가까운 이웃들만 골라내기 때문에 사용합니다.

![L1](https://drive.google.com/uc?export=view&id=1a6fd4eXwUC25BIM6-y2Xc5t165ht7h0z)

이를 실제로 구현한 코드를 보도록 합시다. 가장 먼저 이 방식은 학습할 것이 사실 없습니다. 그저 데이터셋을 불러오는 것이 전부입니다. 그러나 prediction 과정은 다릅니다. 학습과정에서 불러온 전체 데이터셋과 예측할 데이터의 L1 Distance만 구해서 그 중 가장 작은 값을 예측값이라고 하면 되는 것입니다. 그렇기 때문에 데이터셋을 학습시키는 과정은 $O(1)$ 이고, 입력값을 예측하는 과정은 $O(n)$ 입니다.

그러나 이 과정은 실제로 사용하기에는 문제가 있습니다. 실제 사용하기 위해선 학습이 오래걸리더라도 예측이 빨라야 하기 때문입니다. 예를 들어 아이폰을 FaceID로 잠금 해제하는데 1분이 걸리면 아무도 사용하지 않을 것 입니다.
