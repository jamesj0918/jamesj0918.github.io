---
layout: post
title: CS231n 강의 요약 (1) - History of Computer Vision
categories: ["DL"]
tags: ["CS231n"]
---
<br>

## CS231n 강의 요약 (1) - History of Computer Vision

이번 강의에서는 Computer Vision의 전반적인 역사에 대해서 다룹니다.

<br>

### 동물의 시각 정보

우선 Vision의 역사 즉, 지구에서의 '시각'의 역사는 약 5억 4천만 년 전으로 거슬러 올라갑니다. 고고학자들은 약 5억 4천만 년 전, 지구의 생물체의 개체 수가 급증하기 시작했다는 사실을 밝혀냈습니다.
이 현상을 두고 Andrew Parker이라는 호주의 동물학자는 생물체들이 시각에 대한 정보를 받아들이기 시작했기 때문에 일어난 것이라고 설명했습니다. 이후 5억 4천만년이 지난 지금도, 시각 기관은 대부분의 동물들에 가장 중요한 역할을 수행하게 되었습니다. 특히, 인간과 같은 지적 생물체에 대해서는 더더욱 중요합니다.

**이처럼 시각적인 정보는 굉장히 중요합니다.** 이제부터는 컴퓨터 비전이 어떻게 발전해 왔는지에 대해서 살펴보겠습니다.

<br>

### 1. 1959) Hubel & Weisel

![cat brain](https://drive.google.com/uc?export=view&id=1PV9WX1aSIG3FHwFU0L5VGzbZbmP6avDi)

Hubel 과 Weisel이라는 신경생리학자들이 고양이의 뇌에 들어오는 시각 정보에 대해 어떻게 반응하는지에 대한 연구를 했습니다. 이러한 연구들은 이후 컴퓨터 비전에 많은 영향을 끼쳤습니다.
그들은 고양이의 뇌에 전극을 연결시켜 어떠한 시각적인 정보에 크게 반응하는지를 살펴봤습니다. 연구 결과, 고양이의 뇌에서는 움직이는 사물의 가장 기본적인 특징들인 **모서리**들에 크게 반응한다는 것을 알아냅니다.  

이 연구에서 가장 중요한 점은 시각적인 정보의 처리는 **간단한 특징**들로부터 시작하여 **점점 더 크고 구체적인 정보**를 처리한다는 점입니다.

### 2. 1963) Larry Roberts - Block World

![Block World](https://drive.google.com/uc?export=view&id=1yUHcjIeydayNUTaVIrg-T13pWCSUQk42)

이후 1963년에 컴퓨터 비전 역사상 가장 첫 논문이 Larry Roberts에 의해 발표됩니다. 이는 현실 세계의 물체들이 단순한 기하학적인 모양들로 단순하여 이를 기반으로 물체들을 재구성하는 연구였습니다.

### 3. 1966) MIT - The Summer Vision Project

![Summer Vision Project](https://drive.google.com/uc?export=view&id=1L2wLgL8pWo8zE_7ZYb_uURxjnhH1EItY)

The Summer Vision Project는 1966년 MIT에서 진행한 여름 프로젝트로 이 프로젝트의 궁극적인 목표는 사물 인식 (Object Recognition)이였습니다. 이 작은 여름 프로젝트가 50년이 넘게 지난 지금까지 이 문제에 대해서 전 세계의 연구자들이 연구를 활발하게 진행하고 있습니다. 또한 이 분야는 현재 여러 가지 인공지능 분야 중에서 가장 빠르게 성장하고 있는 분야 중 하나이기도 합니다.

### 4. 1970s) David Marr

![David Marr](https://drive.google.com/uc?export=view&id=1Mt8ost6y_8089y_VlQ3a3t-PMLvQvQoo)

1970년대 후반 MIT의 시각연구자 David Marr는 Vision이 어떤 것인지, 이를 컴퓨터 비전의 입장에서 어떻게 알고리즘을 개발하여 컴퓨터가 사진을 인식할 수 있는지에 대한 책을 썼습니다.

그의 생각은 어떠한 사진을 보고 그 사진이 어떤 것인지 인식하려면 여러 가지 과정을 거쳐야한다고 주장했습니다.

    1. Primal Sketch: 사진에서 모서리, 직선, 꼭짓점, 경계점들을 먼저 표현합니다.
    2. 2.5D Sketch: 표면과 깊이 정보, 레이어와 불연속적인 정보들을 추출합니다.
    3. 3D Representation: 이 정보들을 표면과 부피에 대한 정보가 있는 3D 모델링을 합니다.

1번의 내용은 앞서 얘기한 생리학자들의 결론과 같은 맥락인 것을 알 수 있습니다. 이러한 생각은 실제로 이후 몇십 년간 컴퓨터 비전을 지배했습니다.
또한 컴퓨터 비전에 새로 입문하는 학생들에게도 시각적인 정보를 어떻게 분해하여 이해해야 하는지에 대한 직관적인 방식을 제공했습니다.

### 5. 1970s~1980s) Moving on from Block World

![Cylinder](https://drive.google.com/uc?export=view&id=1WHjmCmq3Y4WdGNaOmuOqsrVuWN1XydYj)

1970년대 다른 연구자들은 Block World에서 벗어나 실제 세계의 물체를 어떻게 하면 잘 인식하고 표현할 수 있을지 고민했습니다.
이 당시 컴퓨터들은 매우 느리고 데이터도 충분하지 않은 시절이었습니다. 그럼에도 불구하고 실제 세계의 사물들을 인식하기 위한 방법에 대한 아이디어들이 제시되었습니다.
이 아이디어들의 공통점은 물체의 복잡한 구조를 단순한 기하학적인 형태의 구조들의 집합으로 변환했다는 점입니다.

### 6. 1997) Shi & Malik

![Normalized Cut](https://drive.google.com/uc?export=view&id=1Ug924wT6osQyIkO1Cr-7jPkskbYlOP_D)

1960년대부터 1980년대 까지 컴퓨터가 사물을 인식하는 방법에 대한 여러가지 연구들이 있었지만 이때까지의 연구들은 그저 몇몇 경우에 한정되어 있었습니다.
따라서 실제 세계의 물체들을 표현하는 데에는 한계가 존재했기 떄문에 어려운 사물 인식이 대신 사진을 분할(Image Segmentation)하는 것을 시도합니다.
이 것은 사진의 픽셀들을 의미 있는 부분들로 구분하는 것을 말합니다. 이렇게 모인 그룹들이 무엇인지는 정확하게 모르지만 이를 배경으로부터 분리는 할 수 있었습니다.

### 7. 2001) Face Detection

![Normalized Cut](https://drive.google.com/uc?export=view&id=1ezxEZLJbpC_y9DXYQ4ce4fytBRqZwOte)

한편, 얼굴은 사람들의 세상에서 매우 중요한 부분이기 때문에 얼굴 인식은 다른 사물 인식 분야들보다 훨씬 빠르게 성장했습니다.
2001년, Paul Viola와 Michael Jones는 AdaBoost 알고리즘을 사용하여 실시간 얼굴인식을 완성해냈습니다.
이 연구가 대단했던 이유는 여전히 컴퓨터들은 매우 느렸음에도 불구하고 실시간 얼굴인식이 가능했고, 5년 뒤 Fujifilm의 카메라에서 이 기술을 탑재하여 상용화에 성공했습니다.

### 8. 1990s~2000s) Feature Based

![SIFT](https://drive.google.com/uc?export=view&id=1HUczB0Z_twCwmn86L7SLi_M0Z5-ETEPw)

비슷한 시기, 물체 인식은 해당 물체의 특징을 찾아서 인식하는 방식으로 진행되었습니다.
이 방식의 컨셉은 어떠한 물체를 찍은 이미지는 여러가지 요인(예를 들면 카메라의 각도, 광량, 바라보는 방향 등등)들로 인해 같은 물체일지라도 다르게 보일 수 있습니다.
그러나, 이 물체의 고유한 특징들은 나타날 것이라는 생각을 갖고 이미지를 분석하기 시작했습니다.
따라서 이미지에서 들어난 물체의 주요한 특징들을 추출하여 이 특징들이 비슷한 물체와 비교하는 방식으로 진행됐습니다.
위의 사진의 경우 STOP 표지판의 STOP의 특징들을 비교하는 것을 알 수 있습니다.

### 9. 2000s) Human Recognition

![Human Recognition](https://drive.google.com/uc?export=view&id=1LjpRb8TAaNPEVk9eKlZqOHaKcwmaMvnL)

2000년대에 접어들어서, 사람 인식 분야의 발전속도가 매우 높아졌습니다. 이 방법들도 앞선 방법들과 마찬가지로, 특징점들을(Features) 찾아서 인식하는 방식으로 작동했습니다.
그러나 이전과는 달라진 가장 중요한 점은 바로 **이미지의 품질**이였습니다. 인터넷의 발전과 카메라 기술의 발전으로 인해 컴퓨터 비전을 연구하는데 점점 더 좋은 품질의 데이터가 생긴 것 입니다.

### 10. 2006~2012) Pascal Visual Object Challange

![Pascal](https://drive.google.com/uc?export=view&id=1swGKoKi13WIjZDRVbctsCODA10fm0V0I)

이 시기에, 컴퓨터 비전 분야에서는 중요하고 근본적인 문제가 다시 거론되기 시작했습니다. 바로 사물 인식입니다. 앞서 60년대에 MIT에서 처음 거론된 이후로 이 때까지 이어져 온 것 입니다.
그러나 한 가지 다른 점은, 2000년대에 들어서 사물 인식의 발전을 벤치마킹할 수 있는 데이터셋이 생겼다는 것입니다.
PASCAL(Pattern Analysis, Statistical Modeling and Computational Learning) Visual Object Challenge는 이  데이터셋들중 하나로 사물 인식을 얼마나 정확하게 할 수 있는가를 평가하는 대회였습니다.
이런 대회들에 참가는 전세계의 연구자들로 인해 이 분야가 더 발전할 수 있었습니다.

### 11. 2010~ ) ImageNet

![Imagenet 1](https://drive.google.com/uc?export=view&id=1YDxNlJsw0iKLDFcBdff5EjObLGmE7Tmz)

이후 연구자들은 (이 강의의 지도교수님인 Fei Fei도 포함) 두 가지 생각을 갖고 ImageNet이라는 프로젝트를 시작합니다.

    1. 세상에 존재하는 모든 이미지를 인식해보자.
    2. 데이터의 부족으로 생기는 Overfitting을 없애보자.

이들은 가능한 구할 수 있는 모든 이미지가 있는 세상에서 가장 큰 이미지 데이터셋을 만들어서 이를 학습과 벤치마킹에 사용하고 싶었습니다.
이들은 인터넷에서 수많은 사진들을 다운받아, [WordNet](https://wordnet.princeton.edu/)이라는 사전에 있는 단어들로 구분을 하기 시작했습니다.
이후에 모인 데이터를 정렬하고, 정제하고, 라벨링하는 과정을 걸쳐 ImageNet이라는 약 1500만개의 사진이 22000여개의 카테고리로 나눠져 있는 엄청난 데이터셋이 완성됩니다.

![Imagenet 2](https://drive.google.com/uc?export=view&id=1fxjX-zYb7WS54UyfFUDlozUD1AEN9JT9)
주목할 만한 점은 이 데이터를 갖고 어떻게 이미지 분류 알고리즘들을 벤치마킹할 수 있는가 입니다. 2009년부터 ImageNet팀은 [ImageNet Large-Scale Visual Recognition Challange](http://image-net.org/challenges/LSVRC/)라는 국제 챌린지를 개최합니다.
위의 사진 예시를 보시면 한 사진에 5개의 정답 라벨들이 존재합니다. 이 다섯개를 모두 맞춰야 정답이라고 인정을 합니다.

![Imagenet 4](https://drive.google.com/uc?export=view&id=12vOb6Ipiiwfq4gGg9EsAmpan2xbpEVCo)

2010년부터 2017년까지 이 챌린지의 결과를 보면 매년 오답률이 줄었다는 것을 볼 수 있습니다.
2015년에는 Stanford대학의 박사과정 ~~노예~~ 대학원생이 몇주간 이미지를 분류한 결과보다 더 정확하게 분류한 것입니다.
현실세계에서는 사용하기 어려운 오답률을 가진 알고리즘에서 인간보다 더 뛰어나게 이미지를 분류하는 수준까지 오는 데 불과 몇년밖에 걸리지 않았습니다.

이 그래프에서 주목해야 할 곳은 바로 2011년에서 2012년으로 넘어갔을 때인데, 1년 사이에 오답률을 10%나 획기적으로 줄인 것 입니다.
이 때 사용한 방식이 바로 **Convolutional Neural Networks**입니다. 일명 CNN 혹은 Deep Learning으로 불리기도 합니다. 이는 앞으로 이 강의에서 가장 중요하게 다룰 내용입니다.

![CNN](https://drive.google.com/uc?export=view&id=1xQyY3K0yID6vwAPwEF2zSTu9Vo-oKQk4)

2012년 CNN방식의 알고리즘이 우승한 후, 2017년 대회가 종료될 때 까지 우승 알고리즘은 항상 CNN기반의 알고리즘들이였습니다.
2012년에 우승한 AlexNet(당시에는 SuperVision)은 7개의 레이어를 가진 CNN이였고, 2015년에 우승한 Residual Networks는 152개의 레이어를 가진 CNN이였습니다.

결국 점점 깊이가 깊어지면 성능이 좋아진다는 결론에 도달했습니다. 물론 GPU 메모리의 한계로 무한정 깊이를 깊게할 순 없지만,
시간이 지남에 따라 하드웨어 성능이 좋아지면 알고리즘 성능도 좋아질 수 있다는 뜻이기도 합니다.

![CNN 2](https://drive.google.com/uc?export=view&id=1CRBNudT1rfjDZiNcnv-251R0iPRsWGdI)

이 마법같은 CNN도 하루 아침에 완성된 방식은 아니였습니다. 1998년 손글씨로 쓰여진 숫자나 문자들을 알아볼 수 있는 CNN이 존재했었습니다. 이 구조는 실제로 2012년 ILSVRC를 우승한 AlexNet과 유사한 구조를 갖고있습니다.  
그런데 갑자기 최근들어서 CNN이 유명해진 이유는 1. 컴퓨터의 처리 능력 향상과 2. 많은 양의 데이터가 있었기 때문입니다. [Moore's Law](https://ko.wikipedia.org/wiki/%EB%AC%B4%EC%96%B4%EC%9D%98_%EB%B2%95%EC%B9%99)에 의하면 컴퓨터의 성능은 2년마다 2배로 증가하고 실제로 지금까지도 이 법칙은 잘 맞아왔습니다.
이렇게 급수적으로 성장하는 컴퓨터의 연산능력 차이로 CNN의 학습이 매우 수월해졌고, 또 많은 양의 데이터들이 쌓이기 시작하면서 CNN은 빛을 발하기 시작한 것 입니다.

<br>

**세줄 요약**

    1. CNN 이전에는 이미지에서 특징들을 추출해서 이를 학습하는 방식이 대부분이였다.
    2. 2012년 ILSVRC를 엄청난 차이로 우승한 AlexNet이 등장한 이후로 CNN은 대세가 되었다.
    3. CNN은 컴퓨터의 성능 증가와 데이터 수, 품질이 증가했기 때문에 급성장을 이룰 수 있었다.

<br>

이렇게 전반적인 Computer Vision의 역사에 대해서 알아봤습니다. CISCO에서 발표하길 2020년 전체 인터넷 트래픽의 80%는 비디오 데이터일 것 이라고 발표했는데, 그만큼 컴퓨터 비전이 중요한 분야이고 앞으로 더 중요해질 것 같습니다. 다음 강의부터 본격적으로 시작해보도록 합시다!
