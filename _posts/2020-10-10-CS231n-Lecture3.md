---
layout: post
title: CS231n 강의 요약 (3) - Linear Classifier & Loss Functions
categories: ["DL"]
tags: ["CS231n"]
---

## 1. Linear Classification

지난 시간에는 머신러닝과 이미지 분석에 대한 대략적인 개념과 간단한 예시로 KNN 알고리즘에 대해서 알아보았습니다. 하지만 실제로는 이미지 분석에는 적합하지 않은 알고리즘이였습니다. 이번 시간에는 실제로 앞으로 중요하게 다룰 딥러닝의 기본이 되는 Linear Classification(선형 분류)에 대해서 배워보겠습니다.

### 1.1 Lego Block

![Neural Network](https://drive.google.com/uc?export=view&id=1UxbZIx4miRcWQOteZGjX1h03phSKyIZQ)

우리가 Neural Network을 얘기할 때, 레고 블럭으로 많이 빗댑니다. 여러가지 다른 블럭들이 모여서 하나의 큰 Neural Network, 혹은 이미지 분석 분야에서는 Convolutional Neural Network를 이루게 되는데, 이 레고 블럭들 중 가장 간단하고 기본이 되는 블럭이 바로 Linear Classifier입니다. 그렇기 때문에 이 Linear Classifier를 확실하게 이해하는 것이 중요합니다.

<br>

![Neural Network](https://drive.google.com/uc?export=view&id=17IjBDxvECT84c5mitw38h2IE-kEE-n05)

Linear Classifier가 어떻게 사용될 수 있는지 간단한 예시를 살펴봅시다. 1강에서 언급했던 2012년 ILSVRC를 우승했던 AlexNe t의 구조는 5개의 Convolution layer들 뒤에 3개의 fully-connected layer들로 이루어져있습니다. 여기에서 얘기하는 fully-connected layer들은 Linear Classifier의 일종입니다. 앞서 말한 레고 블럭의 예시로 생각하면 8개의 블럭으로 이루어진 AlexNet에서 3개는 Linear Classifier인 셈입니다.

<br>

### 1.2 Parametric Model

Linear Classifier의 개념에 대해서 알아보겠습니다.
우선 Linear Classifier는 Parametric Model입니다.

Parametric Model이란 주어진 데이터에 대한 정보를 parameter(매개변수)에 저장하는 모델을 뜻합니다. 이러한 모델을 학습시킬 때는 매개변수를 업데이트하고, 예측을 할 때는 학습 데이터는 필요가 없고 매개변수만 이용해서 예측값을 계산합니다.

반면, Non-Parametric Model은 매개변수가 아닌 데이터에 의존합니다. 이전 강의에서 배운 K-Nearest Neighbors가 대표적인 예시입니다. 모델을 학습시킨다기보단 주어진 데이터를 저장했다가 예측할 때는 데이터를 기반으로하는 알고리즘을 적용하여 예측합니다. KNN의 경우에는 가장 가까운 이웃을 찾았죠.

각각의 방식에는 장단점이 있지만 딥러닝에서는 Parametric Model을 사용할 것입니다. Non-Parametric Model의 가장 큰 문제는 KNN의 단점으로도 얘기했었던 예측할 때 시간이 너무 오래걸린다는 것입니다.

<br>

### 1.3 Linear Classifier

![Neural Network](https://drive.google.com/uc?export=view&id=1kjuIU_qoC0uJDygogyN9EUNb_sNVB-LZ)

돌아와서 Linear Classifier를 함수로 생각하면 $$f(x,W)$$으로 일반화할 수 있습니다. 여기에서 $$x$$는 입력 데이터, $$W$$가 Parameter 혹은 Weight로 매개변수 개념입니다. 입력 데이터와 데이터에 대한 정답을 대입하여 매개변수 $$W$$를 업데이트 하는 과정이 바로 모델을 학습하는 과정입니다. $$f$$에는 여러가지 방식이 있겠지만 Linear Classifier는 이름대로 선형이기 때문에 그냥 $$W$$와 $$x$$를 곱하고 편향을 나타내는 상수 개념인 bias $$b$$를 더합니다.

위의 예시를 보겠습니다. 주어진 이미지를 10개의 분류로 분류하는 Linear Classifier를 모델링한다고 생각해봅시다. 우리가 원하는 것은 32x32x3크기의 입력 이미지를 10개의 분류로 나타내는 것입니다. 우선 32x32x3짜리 행렬을 3072x1짜리 벡터 $$x$$로 변환하고, 학습시켜야하는 매개변수 $$W$$는 크기가 10x3072짜리 행렬로 지정합니다. 그러면 계산값인 $$Wx + b$$는 10x1 크기의 행렬로 각각 10개의 분류에 대한 점수를 나타냅니다.

<br>

![Neural Network](https://drive.google.com/uc?export=view&id=1KVf6pkXZa1gS7o13W843p7BGCSJpuEv4)

스케일을 줄여서 좀더 간단한 예시를 보겠습니다.일단 2x2짜리 입력 이미지를 4x1짜리 벡터로 변환합니다. 매개변수 $$W$$와 입력 벡터를 곱하고, 상수 $$b$$를 더해서 결괏값을 계산합니다. 계산된 벡터는 각각 3가지 분류에 대한 모델의 점수를 나타냅니다. 입력이 고양이 사진인데 Dog score가 가장 높으므로 아직 학습이 덜된 것입니다. 학습하는 방법에 대해서는 나중에 자세히 설명하고 이번 강의에서는 Linear Classifier의 개념에 좀더 집중하겠습니다.

<br>

![Neural Network](https://drive.google.com/uc?export=view&id=1M8Z89C_BN972Y0mLy1PBuxuAO5W5S0y4)

학습이 다 된 모델이 데이터를 분류할 때 실제로 무엇을 하는지를 알아보기 위해 학습된 매개변수의 행들을 다시 이미지의 형식으로 변환해봤습니다. 보면 비행기의 점수를 계산하는 행은 푸른 배경에 가운데에도 파란색 물체가 희미하게 보이는 이미지를 나타내는 것 같습니다. 다른 분류의 점수를 계산하는 행들도 비슷합니다. 뚜렷하게 그 물체를 나타내기보단 그 물체들이 평균적으로 갖고있는 특징들을 일반화해서 하나의 필터로써 작용하는 느낌입니다. 하지만 이 예시는 각 분류마다 단 하나의 필터만을 학습하기 때문에 정확도가 낮지만 딥러닝을 더 배우면 하나의 필터만이 아닌 여러 필터들을 활용해서 분류하기 시작하기 때문에 정확도가 점점 높아집니다.

<br>

![Neural Network](https://drive.google.com/uc?export=view&id=1LtpXhHcpACUmDSW2cHn8SxIwXMgOeTi7)

기하적으로 해석하면, 3차원 공간에서 각 이미지들은 특정한 점(벡터)을 나타내고, 이 점들을 효과적으로 구분하는 평면을 구하는 것 입니다. 어차피 계산은 컴퓨터가 해줄 것이기 때문에 이렇게 해석된다라는 것만 알고 넘어가면 될 것 같습니다.


<br>

## 2. Loss Functions

![Neural Network](https://drive.google.com/uc?export=view&id=1A39viJIBnlA1_IE8wahXlCZQqmEfNQZv)

그래서 Linear Classifier가 무슨 일을 하는지 알게되었습니다. 위의 사진에 어떠한 매개변수 $$W$$ 값을 가지는 Linear Classifier로 위의 세 이미지를 입력했을 때 나오는 결과를 살펴봅시다. 각 입력 이미지별로 10개의 분류에 대한 모델의 점수가 나오는데 입력이 고양이 사진일 때 개 분류의 점수가 더 높습니다. 벌써 우리의 모델이 정확하지 않다는 것을 알 수 있습니다.

우리가 궁극적으로 원하는 것은 컴퓨터가 매개변수를 조정해가며 정답 분류에 대한 점수가 가장 높도록 하고싶은데, 이를 하기 위해서는 이 점수가 얼마나 좋은지, 혹은 얼마나 안좋은지 판단하도록 점수들을 일정한 스케일로 정량화할 필요가 있습니다. 이 때 사용되는 것이 바로 **Loss Function** 혹은 직역하면 **손실함수** 입니다. 또한 가능한 여러 매개변수들 중 가장 좋은 매개변수를 찾는 효율적인 방법이 필요한데, 이를 Optimization 이라고 합니다. 이는 이후에 설명하고 Loss Function부터 알아보겠습니다.

<br>

![Neural Network](https://drive.google.com/uc?export=view&id=1ZJJ3xxXF_-2le3fdc4-HiAKjLqGu9Une)

우선 모델에 입력하는 데이터셋이 어떤 모습인지 생각해봅시다. 모델 $$f$$는 $$f(x,W)$$로 정의할 수 있습니다. 여기서 $$x$$는 입력, $$W$$는 매개변수입니다. 데이터셋은 입력 $$x$$와 정답 $$y$$로 주어지고, 이 데이터들을 활용해서 가장 적합한 $$W$$를 찾으면 됩니다. 데이터셋의 크기를 N이라고 하면  $$[x_i, y_i]^N_{i=1}$$ 으로 나타낼 수 있습니다.

위 사진의 예시로는 $$x_i$$는 입력 이미지이고, $$y_i$$는 정답 라벨입니다.(예를 들어 cat, car, dog를 각각 0, 1, 2,로 하면 $$y_i$$는 0~2사이의 정수이겠네요.)

이 모델에 대한 Loss(일반적으로 값이 크면 안좋기 떄문에 Loss 혹은 손실이라고 합니다.)는 이 모델이 얼마나 좋은지 안좋은지 판단해주는 정량화된 지표입니다. Loss를 계산하는 함수들에는 여러가지가 있지만 우선은 $$L_i$$라고 하면 모델의 전체 Loss는 각 분류에 대한 Loss들의 평균 $$L$$이고, 다음과 같이 나타낼 수 있습니다.

$$ L = \frac{1}{N}\sum_i^N{L_i(f(x_i, W),y_i)}$$

Loss Function에 대한 개념은 이미지 분석뿐만 아니라 머신러닝 전반적으로 사용되기 때문에 확실하게 알고 넘어가는게 좋습니다.

### 2.1 Multi-Class SVM Loss

![Neural Network](https://drive.google.com/uc?export=view&id=1BNfoR4_Oa3kvvRtxwMX1ZBAUvsWWpeyP)

첫번째로 알아볼 Loss function은 Multi-Class SVM Loss입니다. Support Vector Machine에서 여러개의 클래스로 확장해서 정의한 함수입니다. SVM까지 설명하며 깊게 들어가진 않겠습니다. Multi-Class SVM Loss의 컨셉은 정답과 정답이 아닌 분류의 점수들의 차가 주어진 마진보다 크게 하는 것 입니다. 식으로 나타내면 다음과 같습니다.

$$L_i = \sum_{j \neq y_i}^{N-1} max(0, s_j - s_{y_i} + M)$$