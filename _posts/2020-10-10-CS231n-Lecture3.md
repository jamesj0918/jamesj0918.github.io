---
layout: post
title: CS231n 강의 요약 (3) - Linear Classifier & Loss Functions
categories: ["DL"]
tags: ["CS231n"]
---

## 1. Linear Classification

지난 시간에는 머신러닝과 이미지 분석에 대한 대략적인 개념과 간단한 예시로 KNN 알고리즘에 대해서 알아보았습니다. 하지만 실제로는 이미지 분석에는 적합하지 않은 알고리즘이였습니다. 이번 시간에는 실제로 앞으로 중요하게 다룰 딥러닝의 기본이 되는 Linear Classification(선형 분류)에 대해서 배워보겠습니다.

### 1.1 Lego Block

![Neural Network](https://drive.google.com/uc?export=view&id=1UxbZIx4miRcWQOteZGjX1h03phSKyIZQ)

우리가 Neural Network을 얘기할 때, 레고 블럭으로 많이 빗댑니다. 여러가지 다른 블럭들이 모여서 하나의 큰 Neural Network, 혹은 이미지 분석 분야에서는 Convolutional Neural Network를 이루게 되는데, 이 레고 블럭들 중 가장 간단하고 기본이 되는 블럭이 바로 Linear Classifier입니다. 그렇기 때문에 이 Linear Classifier를 확실하게 이해하는 것이 중요합니다.

<br>

![Neural Network](https://drive.google.com/uc?export=view&id=17IjBDxvECT84c5mitw38h2IE-kEE-n05)

Linear Classifier가 어떻게 사용될 수 있는지 간단한 예시를 살펴봅시다. 1강에서 언급했던 2012년 ILSVRC를 우승했던 AlexNet의 구조는 5개의 Convolution layer들 뒤에 3개의 fully-connected layer들로 이루어져있습니다. 여기에서 얘기하는 fully-connected layer들은 Linear Classifier의 일종입니다. 앞서 말한 레고 블럭의 예시로 생각하면 8개의 블럭으로 이루어진 AlexNet에서 3개는 Linear Classifier인 셈입니다.

<br>

### 1.2 Parametric Model

Linear Classifier의 개념에 대해서 알아보겠습니다.
우선 Linear Classifier는 Parametric Model입니다.

Parametric Model이란 주어진 데이터에 대한 정보를 parameter(매개변수)에 저장하는 모델을 뜻합니다. 이러한 모델을 학습시킬 때는 매개변수를 업데이트하고, 예측을 할 때는 학습 데이터는 필요가 없고 매개변수만 이용해서 예측값을 계산합니다.

반면, Non-Parametric Model은 매개변수가 아닌 데이터에 의존합니다. 이전 강의에서 배운 K-Nearest Neighbors가 대표적인 예시입니다. 모델을 학습시킨다기보단 주어진 데이터를 저장했다가 예측할 때는 데이터를 기반으로하는 알고리즘을 적용하여 예측합니다. KNN의 경우에는 가장 가까운 이웃을 찾았죠.

각각의 방식에는 장단점이 있지만 딥러닝에서는 Parametric Model을 사용할 것입니다. Non-Parametric Model의 가장 큰 문제는 KNN의 단점으로도 얘기했었던 예측할 때 시간이 너무 오래걸린다는 것입니다.

<br>

![Neural Network](https://drive.google.com/uc?export=view&id=1kjuIU_qoC0uJDygogyN9EUNb_sNVB-LZ)

돌아와서 Linear Classifier를 함수로 생각하면 $$f(x,W)$$으로 일반화할 수 있습니다. 여기에서 $$x$$는 입력 데이터, $$W$$가 Parameter 혹은 Weight로 매개변수 개념입니다. 입력 데이터와 데이터에 대한 정답을 대입하여 매개변수 $$W$$를 업데이트 하는 과정이 바로 모델을 학습하는 과정입니다. $$f$$에는 여러가지 방식이 있겠지만 Linear Classifier는 이름대로 선형이기 때문에 그냥 $$W$$와 $$x$$를 곱하고 편향을 나타내는 상수 개념인 bias $$b$$를 더합니다.

위의 예시를 보겠습니다. 주어진 이미지를 10개의 분류로 분류하는 Linear Classifier를 모델링한다고 생각해봅시다. 우리가 원하는 것은 32x32x3크기의 입력 이미지를 10개의 분류로 나타내는 것입니다. 우선 32x32x3짜리 행렬을 3072x1짜리 벡터 $$x$$로 변환하고, 학습시켜야하는 매개변수 $$W$$는 크기가 10x3072짜리 행렬로 지정합니다. 그러면 계산값인 $$Wx + b$$는 10x1 크기의 행렬로 각각 10개의 분류에 대한 점수를 나타냅니다.

<br>

![Neural Network](https://drive.google.com/uc?export=view&id=1KVf6pkXZa1gS7o13W843p7BGCSJpuEv4)

스케일을 줄여서 좀더 간단한 예시를 보겠습니다.일단 2x2짜리 입력 이미지를 4x1짜리 벡터로 변환합니다. 매개변수 $$W$$와 입력 벡터를 곱하고, 상수 $$b$$를 더해서 결괏값을 계산합니다. 계산된 벡터는 각각 3가지 분류에 대한 모델의 점수를 나타냅니다. 입력이 고양이 사진인데 Dog score가 가장 높으므로 아직 학습이 덜된 것입니다. 학습하는 방법에 대해서는 나중에 자세히 설명하고 이번 강의에서는 Linear Classifier의 개념에 좀더 집중하겠습니다.

<br>

![Neural Network](https://drive.google.com/uc?export=view&id=1M8Z89C_BN972Y0mLy1PBuxuAO5W5S0y4)

학습이 다 된 모델이 데이터를 분류할 때 실제로 무엇을 하는지를 알아보기 위해 학습된 매개변수의 행들을 다시 이미지의 형식으로 변환해봤습니다. 보면 비행기의 점수를 계산하는 행은 푸른 배경에 가운데에도 파란색 물체가 희미하게 보이는 이미지를 나타내는 것 같습니다. 다른 분류의 점수를 계산하는 행들도 비슷합니다. 뚜렷하게 그 물체를 나타내기보단 그 물체들이 평균적으로 갖고있는 특징들을 일반화해서 하나의 필터로써 작용하는 느낌입니다. 하지만 이 예시는 각 분류마다 단 하나의 필터만을 학습하기 때문에 정확도가 낮지만 딥러닝을 더 배우면 하나의 필터만이 아닌 여러 필터들을 활용해서 분류하기 시작하기 때문에 정확도가 점점 높아집니다.

<br>

![Neural Network](https://drive.google.com/uc?export=view&id=1LtpXhHcpACUmDSW2cHn8SxIwXMgOeTi7)

기하적으로 해석하면, 3차원 공간에서 각 이미지들은 특정한 점(벡터)을 나타내고, 이 점들을 효과적으로 구분하는 평면을 구하는 것 입니다. 어차피 계산은 컴퓨터가 해줄 것이기 때문에 이렇게 해석된다라는 것만 알고 넘어가면 될 것 같습니다.